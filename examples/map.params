#!/bin/bash

: "Required parameters
    - BOX_MNT_PNT : where data can be read/written to:
    - SAMPLE_NAME : Sample name to be applied to the headers and filename of BAM.
    - INPUT_DIR   : Folder containing input data.  This most only contain data for this sample.
                    Can be:
                     - multiple BAM inputs ($INPUT_DIR/*.bam)
                     - multiple CRAM inputs ($INPUT_DIR/*.cram)
                     - mixture of BAM + CRAM inputs ($INPUT_DIR/*.*am)
                     - multiple paired FASTQ ($INPUT_DIR/*_[12].fq[.gz])
                     - multiple interleaved FASTQ ($INPUT_DIR/*.fq[.gz])
    - REF_BASE    : Folder containing genome.fa, genome.dict and associated index files:
                     - If supporting files are absent they will be built where possible.
                     - genome.dict - see `samtools dict`
    - CRAM        : 0=BAM 1=CRAM
    - SCRAMBLE    : Parameters to pass to scramble when 'CRAM=1'
 "

BOX_MNT_PNT='/datastore' # do not change in Docker execution
SAMPLE_NAME='COLO-829'
INPUT_DIR=$BOX_MNT_PNT/inputs
CRAM=0

: "Optionally force the CPU count to this value, by default all cores available are used at
   appropriate points in the flow"
#CPU=4

TIMEZONE='Europe/London'

: "PRE_EXEC - Commands to be run before the main workflow starts
    - NOTE: To allow many commands to run an array is defined, the first command must
            be assigned to 'PRE_EXEC[0]=' all following having the number in '[]' incremented by 1
            Order of addition is maintaned.
    - Can be used to stage your files, the above variables are imported before this so
      they can be used in the normal fashion, $NAME_WT etc
    - This example pulls an arcive of CRAM data, upacks, converts to BAM and builds indexes.
      The following describes the individual commands
        0. Cleans any existing data to remove partial setup
        1. creates the input directory in the mounted volume (root is always /datastore in docker)
        2. Downloads the downsampled test data
        3. Unpacks the data into the $BOX_MNT_PNT/input area
        4. Delete the tar to minimise required space
        5. Converts the MT cram file to bam
        6. Deletes the MT cram file to minimise required space
        7. Converts the WT cram file to bam
        8. Deletes the WT cram file to minimise required space"

# PRE_EXEC[0]="rm -rf $BOX_MNT_PNT/input $BOX_MNT_PNT/testdata.tar"
# PRE_EXEC[1]="mkdir $BOX_MNT_PNT/input"
# PRE_EXEC[2]="curl -sSL --retry 10 -o $BOX_MNT_PNT/testdata.tar ftp://ftp.sanger.ac.uk/pub/cancer/cgpbox/testdata-${CGPBOX_VERSION}.tar"
# PRE_EXEC[3]="tar -C $BOX_MNT_PNT/input --strip-components 1 -xf $BOX_MNT_PNT/testdata.tar"
# PRE_EXEC[4]="rm -f $BOX_MNT_PNT/testdata.tar"
# PRE_EXEC[5]="bamcollate2 inputformat=cram outputformat=bam collate=0 index=1 outputthreads=$CPU exclude= filename=$BOX_MNT_PNT/input/${NAME_MT}.cram O=$BOX_MNT_PNT/input/${NAME_MT}.bam indexfilename=$BOX_MNT_PNT/input/${NAME_MT}.bam.bai"
# PRE_EXEC[6]="rm -f $BOX_MNT_PNT/input/${NAME_MT}.cram"
# PRE_EXEC[7]="bamcollate2 inputformat=cram outputformat=bam collate=0 index=1 outputthreads=$CPU exclude= filename=$BOX_MNT_PNT/input/${NAME_WT}.cram O=$BOX_MNT_PNT/input/${NAME_WT}.bam indexfilename=$BOX_MNT_PNT/input/${NAME_WT}.bam.bai"
# PRE_EXEC[8]="rm -f $BOX_MNT_PNT/input/${NAME_WT}.cram"

: "Same as PRE_EXEC but run at the end of the workflow
    - This is an example how you could write your result to S3 by setting the environment variables and path for your bucket
    - Nice use is that you can then set any AWS image to shutdown automatically when it becomes idle"

#POST_EXEC[0]='export AWS_ACCESS_KEY_ID=XXX'
#POST_EXEC[1]='export AWS_SECRET_ACCESS_KEY=YYY'
#POST_EXEC[2]='export AWS_DEFAULT_REGION=ZZZ'
#POST_EXEC[3]='aws s3 cp result_${NAME_MT}_vs_${NAME_WT}.tar.gz s3://some-bucket/result_${NAME_MT}_vs_${NAME_WT}.tar.gz'
