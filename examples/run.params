#!/bin/bash

: "Required parameters
    - NAME_* : should match that in the BAM file headers, primarily for naming of folders
    - *_MT   : Refers to the Tumour sample
    - *_WT   : Refers to the Wildtype/normal sample"

NAME_MT='HCC1143'
NAME_WT='HCC1143_BL'
BAM_MT='/datastore/input/HCC1143.bam'
BAM_WT='/datastore/input/HCC1143_BL.bam'

: "PRE_EXEC - Commands to be run before the main workflow starts
    - NOTE: To allow many commands to run an array is defined, the first command must
            be assigned to 'PRE_EXEC[0]=' all following having the number in '[]' incremented by 1
            Order of addition is maintaned.
    - Can be used to stage your files, the above variables are imported before this so
      they can be used in the normal fashion, $NAME_WT etc
    - This example
        1. creates the input directory in the mounted volume (root is always /datastore in docker)
        2. Downloads the downsampled test data
        3. Unpacks the data into the /datastore/input area
        4. Removes the *.bas files to force generation"

PRE_EXEC[0]='rm -rf /datastore/input'
PRE_EXEC[1]='mkdir /datastore/input'
PRE_EXEC[2]='curl -sSL --retry 10 -o /datastore/testdata.tar https://s3-eu-west-1.amazonaws.com/wtsi-pancancer/testdata/HCC1143_ds.tar'
PRE_EXEC[3]='tar -C /datastore/input --strip-components 1 -xf /datastore/testdata.tar'
PRE_EXEC[4]='rm -f /datastore/input/*.bas'

: "Same as PRE_EXEC but run at the end of the workflow
    - This is an example how you could write your result to S3 by setting the environment variables and path for your bucket
    - Nice use is that you can then set any AWS image to shutdown automatically when it becomes idle"

POST_EXEC[0]='export AWS_ACCESS_KEY_ID=XXX'
POST_EXEC[1]='export AWS_SECRET_ACCESS_KEY=YYY'
POST_EXEC[2]='export AWS_DEFAULT_REGION=ZZZ'
POST_EXEC[3]='tar -C /datastore -zcf result_${NAME_MT}_vs_${NAME_WT}.tar.gz output'
POST_EXEC[4]='aws s3 cp result_${NAME_MT}_vs_${NAME_WT}.tar.gz s3://wtsi-pancancer/kr2Test/result_${NAME_MT}_vs_${NAME_WT}.tar.gz'
